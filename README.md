# ğŸ“ˆ Stock Prediction MLOps System

**Live Dashboard:** [https://stockpredictionmlops-jxahbuhq2jqqu464mgqyul.streamlit.app/](https://stockpredictionmlops-jxahbuhq2jqqu464mgqyul.streamlit.app/)

**API Endpoint:** [https://stockpredictionmlops-production.up.railway.app/](https://stockpredictionmlops-production.up.railway.app/)

**Experiment Tracking:** [https://api.wandb.ai/links/hanseeka-dhingana-sukkur-iba-university/vgs0jzg2](https://api.wandb.ai/links/hanseeka-dhingana-sukkur-iba-university/vgs0jzg2)

---

## ğŸ“‹ Table of Contents

* [Project Overview](#-project-overview)
* [System Architecture](#ï¸-system-architecture)
* [Key Features](#-key-features)
* [Technology Stack](#ï¸-technology-stack)
* [Installation & Setup](#-installation--setup)
* [Project Structure](#-project-structure)
* [MLOps Pipelines (CI/CD/CT)](#-mlops-pipelines-cicdct)
* [API Documentation](#-api-documentation)
* [Performance & Results](#-performance--results)

---

## ğŸ¯ Project Overview

**Stock Prediction MLOps** is an end-to-end Machine Learning system designed to predict the next day's stock price using historical market data. Unlike static notebooks, this project implements a full **MLOps lifecycle**: automated data ingestion, a centralized feature store, continuous training (CT), and automated deployment.

### ğŸ† Objectives Achieved

* âœ… **Automated Data Pipeline:** Live data fetching from Yahoo Finance (`yfinance`).
* âœ… **Feature Store:** Centralized storage of engineered features using **MongoDB Atlas**.
* âœ… **Experiment Tracking:** Model metrics and artifacts tracked via **Weights & Biases (WandB)**.
* âœ… **Continuous Training (CT):** Weekly automated retraining via GitHub Actions to adapt to new market trends.
* âœ… **Continuous Integration (CI):** Automated unit testing with `pytest`.
* âœ… **Continuous Deployment (CD):** Auto-deployment to **Railway** (Backend) and **Streamlit** (Frontend).

---

## ğŸ—ï¸ System Architecture

### High-Level Data Flow

```ascii
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Yahoo Financeâ”‚â”€â”€â”€â”€â–¶â”‚  Data Ingestion  â”‚â”€â”€â”€â”€â–¶â”‚  Feature Store  â”‚
â”‚     API      â”‚     â”‚   & Processing   â”‚     â”‚ (MongoDB Atlas) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit   â”‚â—€â”€â”€â”€â–¶â”‚ FastAPI Service  â”‚â—€â”€â”€â”€â”€â”‚ Model Training  â”‚
â”‚  Dashboard   â”‚     â”‚    (Railway)     â”‚     â”‚    (GitHub)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚ Experiment Logs â”‚
                                              â”‚     (WandB)     â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### Component Details

1. **Data Layer:** Fetches raw stock data, calculates Technical Indicators (SMA, Volatility), and pushes them to **MongoDB**.
2. **ML Layer:** An Ensemble Model (Linear Regression + Random Forest + XGBoost) trained on data pulled from the Feature Store.
3. **Ops Layer:** GitHub Actions handles the "Self-Learning" loop (Retraining) and "Quality Gate" (Testing).
4. **Serving Layer:** FastAPI provides a REST endpoint for predictions; Streamlit consumes this API for the UI.

---

## âœ¨ Key Features

### 1. ğŸ§  Automated Data Pipeline (Continuous Training)

The system doesn't just sit there. Every **Sunday at Midnight**, a GitHub Action:

1. Wakes up and downloads fresh data from Yahoo Finance.
2. Updates the Feature Store (MongoDB).
3. Retrains the Ensemble Model.
4. Commits the new model back to the repository.

### 2. ğŸ—„ï¸ MongoDB Feature Store

Instead of relying on static CSVs, processed features (`SMA_10`, `SMA_50`, `Volatility`) are stored in MongoDB. This ensures training and inference always use the exact same feature definitions.

### 3. ğŸ§ª Robust CI/CD Pipeline

* **CI (Continuous Integration):** Every code push triggers `pytest` to ensure the API and Model logic aren't broken.
* **CD (Continuous Deployment):** If tests pass, Railway automatically builds and deploys the new API version.

---

## ğŸ› ï¸ Technology Stack

| Category | Technologies |
| --- | --- |
| **Language** | Python 3.12 |
| **Machine Learning** | Linear Regression, Random Forest, XGBoost, Ensemble Learning (Voting Regressor), Scikit-Learn, Pandas, NumPy |
| **Backend API** | FastAPI, Uvicorn, Pydantic |
| **Frontend** | Streamlit|
| **Database** | MongoDB Atlas (Feature Store) |
| **MLOps Tools** | GitHub Actions, Weights & Biases (WandB), Docker |
| **Deployment** | Railway (API), Streamlit Cloud (UI) |

---

## ğŸ“¦ Installation & Setup

### Prerequisites

* Python 3.10+
* MongoDB Atlas Account
* WandB Account

### Step 1: Clone Repository

```bash
git clone https://github.com/Hanseeka-Dhingana/stock_prediction_mlops.git
cd stock_prediction_mlops

```

### Step 2: Create Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt

```

### Step 4: Configure Environment (`.env`)

Create a `.env` file in the root directory:

```ini
MONGO_URI="mongodb+srv://<username>:<password>@cluster.mongodb.net/..."
WANDB_API_KEY="wd-..."

```

### Step 5: Run Locally

**1. Run the Pipeline (Ingest -> Train):**

```bash
python src/pipeline/train_pipeline.py

```

**2. Start the API:**

```bash
uvicorn api.main:app --reload

```

**3. Start the Dashboard:**

```bash
streamlit run frontend/app.py

```

---

## ğŸ“ Project Structure

```text
stock_prediction_mlops/
â”œâ”€â”€ .github/workflows/       # The Brains (Automation)
â”‚   â”œâ”€â”€ ci.yml               # Runs Tests on every push
â”‚   â””â”€â”€ retrain.yml          # Weekly Self-Learning Robot
â”œâ”€â”€ api/                     # Backend
â”‚   â”œâ”€â”€ main.py              # FastAPI endpoints
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/                    # csv files of dataset
â”œâ”€â”€ frontend/                # Frontend
â”‚   â””â”€â”€ app.py               # Streamlit Dashboard
â”œâ”€â”€ models/               # Local storage for Models
â”œâ”€â”€ notebook/                # perform EDA and train model
|   â”œâ”€â”€ EDA_stock_prediction.ipynb
|   â””â”€â”€ model_training.ipynb
â”œâ”€â”€ src/                     # Core Logic
|   â”œâ”€â”€ __init__.py 
â”‚   â”œâ”€â”€ components/
|   |   â”œâ”€â”€ __init__.py 
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py      # Yahoo Finance -> CSV
â”‚   â”‚   â”œâ”€â”€ data_transformation.py # CSV -> MongoDB Features
â”‚   â”‚   â””â”€â”€ model_trainer.py       # MongoDB -> Models
â”‚   â”œâ”€â”€ pipeline/
|   |   â”œâ”€â”€ __init__.py 
â”‚   â”‚   â””â”€â”€ train_pipeline.py      # Orchestrator
|   â”œâ”€â”€ exception.py
|   â”œâ”€â”€ logger.py
|   â”œâ”€â”€ tunning.py           # train models on different hyperparameters
|   â””â”€â”€ utils.py
â”œâ”€â”€ tests/                   # Unit Tests
|   â”œâ”€â”€ __init__.py 
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ gitignore
â”œâ”€â”€ export_results.py
â”œâ”€â”€ setup.py
â”œâ”€â”€ sweep_results.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

---

## ğŸ”„ MLOps Pipelines (CI/CD/CT)

### 1. Continuous Integration (CI)

* **Trigger:** Push to `main`.
* **Job:** Runs unit tests (`pytest`). Checks if API endpoints return 200 OK.
* **File:** `.github/workflows/ci.yml`

### 2. Continuous Training (CT)

* **Trigger:** Schedule (Every Sunday 00:00) OR Manual Dispatch.
* **Job:**  Fetches new data.
* Retrains model.
* Logs performance to **WandB**.
* **Commits** the new `model.pkl` to the repo.


* **File:** `.github/workflows/retrain.yml`

### 3. Continuous Deployment (CD)

* **Trigger:** When `model.pkl` is updated in the repo.
* **Job:** Railway detects the change, rebuilds the Docker container, and updates the live API.

---

## ğŸ“¡ API Documentation

**Base URL:** `https://your-railway-app-url.app`

### 1. Health Check

`GET /`
Returns status of the API.

```json
{ "status": "active", "model": "v1.0" }

```

### 2. Predict Price

`POST /predict`
Predicts the next day's closing price.

**Request:**

```json
{
  "Close": 150.5,
  "SMA_10": 148.2,
  "SMA_50": 145.0,
  "Volatility": 2.1
}

```

**Response:**

```json
{
  "status": "success",
  "predicted_price": 152.34
}

```

---

## ğŸ“Š Performance & Results

We optimized our hyperparameters using a **Weights & Biases Sweep**.
* **Best Run:** `swept-sweep-5`
* **Best MAE:** 21.83
* **Best Hyperparameters:**
  * **Random Forest:** `n_estimators=200`, `max_depth=10`
  * **XGBoost:** `n_estimators=200`, `learning_rate=0.2`

*Check the [WandB Dashboard](https://api.wandb.ai/links/hanseeka-dhingana-sukkur-iba-university/vgs0jzg2) for the full sweep comparison.*


---

### ğŸ‘¤ Author

**Hanseeka Dhingana**